# AI-Powered News Summarization API

*(FastAPI + HuggingFace Transformers)*

## Overview

This project is a **production-style REST API** that performs **abstractive news summarization** using a state-of-the-art Transformer model (**BART**).
Users send a news article as input, and the API returns a **short, coherent summary** generated by an AI model.

The system is designed like a real backend ML service, with:

* preprocessing
* model serving
* inference metrics
* caching
* health monitoring
* clean API design

---

## What problem does this solve?

Reading long news articles is time-consuming. This API automatically:

* understands long news text
* extracts the core meaning
* rewrites it into a concise summary

This is **abstractive summarization**, meaning:

> The summary is **not copied** from the article — it is **generated in new words**.

---

## High-Level Architecture

```
User Text
   ↓
Preprocessing (cleaning, limits)
   ↓
Tokenizer (converts text → tokens)
   ↓
Transformer Model (BART)
   ↓
Generated Summary
   ↓
JSON API Response
```

Optionally, results are cached using Redis to reduce repeated inference time.

---

## API Endpoints Explained

The API exposes **two main endpoints**:

---

### 1️ Health Check Endpoint

`GET /v1/health`

#### Purpose

This endpoint is used to **check whether the service is alive and correctly configured**.

It does **not** run the ML model.
It is meant for:

* monitoring
* debugging
* deployment checks
* load balancers

#### Example Response

```json
{
  "status": "ok",
  "model": "facebook/bart-large-cnn",
  "cache_enabled": true
}
```

#### What each field means

| Field           | Meaning                                         |
| --------------- | ----------------------------------------------- |
| `status`        | API is running correctly                        |
| `model`         | Name of the ML model currently loaded in memory |
| `cache_enabled` | Whether Redis caching is enabled                |

If this endpoint returns `200 OK`, the backend and configuration are correct.

---

### 2️ Summarization Endpoint

`POST /v1/summarize`

#### Purpose

This is the **main business endpoint**.
It accepts a news article and returns an AI-generated summary.

---

## How Summarization Works (Internally)

When you send text to `/v1/summarize`, the following happens:

1. **Validation**

   * Ensures text exists
   * Ensures text is long enough to summarize

2. **Preprocessing**

   * Removes extra whitespace
   * Removes control characters
   * Enforces maximum length limits

3. **Tokenization**

   * Converts text into numerical tokens
   * Applies truncation if needed

4. **Model Inference**

   * Uses `facebook/bart-large-cnn`
   * Applies beam search decoding
   * Prevents repeated phrases
   * Generates a concise summary

5. **Metrics Collection**

   * Token counts
   * Latency (ms)
   * Compression ratio

6. **Caching (Optional)**

   * If the same text is sent again, the cached result is returned instantly

---

## Example Request

### Request Body

```json
{
  "text": "Apple announced new updates to its iPhone lineup today, focusing on improved battery life and faster on-device AI features. The company said the new chip design improves efficiency and performance. Analysts expect the updates to drive upgrades, especially among users with older phones."
}
```

---

## Example Response

```json
{
  "summary": "Apple announced updates to its iPhone lineup emphasizing improved battery life, enhanced on-device AI features, and better performance, which analysts believe will drive upgrades.",
  "truncated": false,
  "model_name": "facebook/bart-large-cnn",
  "device": "cpu",
  "input_tokens": 72,
  "output_tokens": 44,
  "latency_ms": 4773,
  "cache_hit": false,
  "compression_ratio": 0.59
}
```

---

## Response Fields Explained

| Field               | Description                                     |
| ------------------- | ----------------------------------------------- |
| `summary`           | AI-generated abstractive summary                |
| `truncated`         | Whether input text was cut due to length limits |
| `model_name`        | Model used for inference                        |
| `device`            | CPU or GPU                                      |
| `input_tokens`      | Number of tokens in input                       |
| `output_tokens`     | Number of tokens in summary                     |
| `latency_ms`        | End-to-end inference time                       |
| `cache_hit`         | Whether result came from Redis cache            |
| `compression_ratio` | Summary length / original text length           |

---

## How to Run the Project (Step-by-Step)

### 1️ Create virtual environment

```bash
python -m venv .venv
.venv\Scripts\Activate.ps1   # Windows
```

### 2️ Install dependencies

```bash
pip install -r requirements.txt
```

### 3️ Create environment config

```bash
copy .env.example .env
```

### 4 Start the server

```bash
python -m uvicorn app.main:app --reload
```

You should see:

```
INFO: Model loaded and ready.
INFO: Application startup complete.
```

---

## How to Check If It Works

### Option 1: Swagger UI (Recommended)

Open browser:

```
http://127.0.0.1:8000/docs
```

1. Test `GET /v1/health`
2. Test `POST /v1/summarize`
3. Paste a news article
4. Click **Execute**

If you receive a valid summary → it works 

---

### Option 2: Command Line (curl)

```bash
curl -X POST http://127.0.0.1:8000/v1/summarize \
  -H "Content-Type: application/json" \
  -d '{"text":"Paste a news article here"}'
```

---

## How to Verify Caching Works

1. Send a summarization request
2. Send **the same request again**
3. Observe:

```json
"cache_hit": true
```

This confirms Redis caching is active and reducing latency.

---

## Why `facebook/bart-large-cnn`?

* Pretrained and fine-tuned specifically for **news summarization**
* Trained on CNN/DailyMail dataset
* Produces high-quality abstractive summaries
* Widely used as an industry baseline

The model is configurable via `.env`:

```env
MODEL_NAME=facebook/bart-large-cnn
```

---

## When to Use This API

* News aggregation apps
* Content summarization tools
* Research pipelines
* Backend ML services
* Learning production ML deployment

---

## Key Takeaways

* This is a **real ML backend service**, not a toy script
* Follows clean architecture and production patterns
* Demonstrates ML + backend + system design together
* Fully testable and extensible

---

## Short Summary

Built as a production-ready ML microservice using FastAPI and HuggingFace Transformers.


